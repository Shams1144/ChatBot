{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzisjEJPUjuR",
        "outputId": "07640335-2015-40f2-90ac-df84522e4311"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import json\n",
        "import random\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-hCuwLWVVGZI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXN2Vh9IVQm-",
        "outputId": "2ebc6662-a751-4044-e825-c1f7b152c6f0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load intents data from JSON\n",
        "with open(\"/content/drive/MyDrive/Pdf/train_freq.json\", \"r\") as f:\n",
        "    intents = json.load(f)\n"
      ],
      "metadata": {
        "id": "H4ApL4r7VT8D"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Preprocess data\n",
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_letters = ['?', '!', '.', ',']\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['text']:\n",
        "        word_list = nltk.word_tokenize(pattern)\n",
        "        words.extend(word_list)\n",
        "        documents.append((word_list, intent['intent']))\n",
        "        if intent['intent'] not in classes:\n",
        "            classes.append(intent['intent'])\n",
        "\n",
        "words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in ignore_letters]\n",
        "words = sorted(set(words))\n",
        "classes = sorted(set(classes))\n",
        "\n",
        "pickle.dump(words, open('words.pkl', 'wb'))\n",
        "pickle.dump(classes, open('classes.pkl', 'wb'))\n",
        "\n"
      ],
      "metadata": {
        "id": "cPhk4BWAVXiW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training data\n",
        "training = []\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "for document in documents:\n",
        "    bag = []\n",
        "    word_patterns = document[0]\n",
        "    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
        "\n",
        "    for word in words:\n",
        "        bag.append(1) if word in word_patterns else bag.append(0)\n",
        "\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(document[1])] = 1\n",
        "    training.append([bag, output_row])\n",
        "\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "train_x = list(training[:, 0])\n",
        "train_y = list(training[:, 1])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zsfr9GmpVbOB",
        "outputId": "4424f269-3641-4bc7-90f9-5746ab60cd75"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-cab24e05dabe>:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  training = np.array(training)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and train the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "\n"
      ],
      "metadata": {
        "id": "5Bgx0Rr-ViEO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the legacy SGD optimizer\n",
        "sgd = tf.keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=0)\n",
        "model.save('chatbot_model.h5', hist)\n",
        "print('Model training completed.')\n",
        "\n",
        "# Load the trained model and other resources\n",
        "intents = json.loads(open('/content/drive/MyDrive/Pdf/train_freq.json').read())\n",
        "words = pickle.load(open('words.pkl', 'rb'))\n",
        "classes = pickle.load(open('classes.pkl', 'rb'))\n",
        "model = tf.keras.models.load_model('chatbot_model.h5')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AnrSJNBVmxd",
        "outputId": "006cf985-eecf-41be-d654-6435691c7c97"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define functions for message processing and response generation\n",
        "def clean_up_sentence(sentence):\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
        "    return sentence_words\n",
        "\n",
        "def bag_of_words(sentence):\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    bag = [0] * len(words)\n",
        "    for w in sentence_words:\n",
        "        for i, word in enumerate(words):\n",
        "            if word == w:\n",
        "                bag[i] = 1\n",
        "    return np.array(bag)\n",
        "\n",
        "def predict_class(sentence):\n",
        "    bow = bag_of_words(sentence)\n",
        "    res = model.predict(np.array([bow]))[0]\n",
        "    ERROR_THRESHOLD = 0.25\n",
        "    results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append({'intent': classes[r[0]], 'probability': str(r[1])})\n",
        "    return return_list\n",
        "\n",
        "def get_response(intents_list, intents_json):\n",
        "    tag = intents_list[0]['intent']\n",
        "    list_of_intents = intents_json['intents']\n",
        "    for i in list_of_intents:\n",
        "        if i['intent'] == tag:\n",
        "            result = random.choice(i['responses'])\n",
        "            break\n",
        "    return result\n",
        "\n"
      ],
      "metadata": {
        "id": "WC818b2pVs3O"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat loop\n",
        "print(\"GO! Bot is running! Type 'quit' to close the chatbot.\")\n",
        "\n",
        "while True:\n",
        "    message = input(\"You: \")\n",
        "    if message.lower() == 'quit':\n",
        "        print('Bot: Bye, see you again!')\n",
        "        break\n",
        "    else:\n",
        "        ints = predict_class(message)\n",
        "        res = get_response(ints, intents)\n",
        "        print(f'Bot: {res}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoHMCFXxWSLv",
        "outputId": "db77f1fa-aa0d-4816-c2a8-a2b354add37b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GO! Bot is running! Type 'quit' to close the chatbot.\n",
            "You: hello\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Bot: Hello human, please tell me your Friend user\n",
            "You: what is your name\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Bot: Call me Friend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M2exOtGrWUvM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}